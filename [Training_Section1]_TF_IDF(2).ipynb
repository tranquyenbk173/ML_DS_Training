{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Training_Section1] TF-IDF",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uqff0b7J1mM",
        "colab_type": "code",
        "outputId": "6ac5984d-4e05-471f-dc98-f304e46a5205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/DS_Lab/Data/"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DS_Lab/Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRuTgE8fDKYE",
        "colab_type": "text"
      },
      "source": [
        "#Tiền xử lí: thu thập dữ liệu - Loại bỏ stop words & stemming --> Luu kq vào file:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rCPUHlJLCIH",
        "colab": {}
      },
      "source": [
        "#lay danh sach cac thu muc va newsgroup:\n",
        "from os import listdir\n",
        "def get_list_dirs_and_newsgroup():\n",
        "  path = '20news-bydate/'\n",
        "  parent_dirs = [path + dir_name + '/' for dir_name in listdir(path)]\n",
        "  train_dir, test_dir = (parent_dirs[0], parent_dirs[1]) if 'train' in parent_dirs[0] else (parent_dirs[1], parent_dirs[0])\n",
        "  list_group_name = [group_name for group_name in listdir(train_dir)] #day la danh sach cac group chung cho ca tap train va tap test\n",
        "  list_group_name.sort()\n",
        "  \n",
        "  return train_dir, test_dir, parent_dirs, list_group_name\n",
        "\n",
        "#Lay ra stop_words: \n",
        "def get_stop_words():\n",
        "  with open(path + 'stop_word.txt') as file:\n",
        "    stop_words = file.read().splitlines()\n",
        "  file.close()\n",
        "  return stop_words\n",
        "\n",
        "#Import REgular Expression:\n",
        "import re\n",
        "#Import stemming package:\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#ham lay du lieu tu tap train va tap test:\n",
        "def collect_data(parent_dir, list_group_name):\n",
        "  data = [] #chua du lieu sau khi thu thap - moi van ban la mot phan tu\n",
        "  list_group_name = get_list_dirs_and_newsgroup()[3]\n",
        "  stop_words = get_stop_words()\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  #Xu li voi tung group trong list:\n",
        "  for group_id, group_name in enumerate(list_group_name):\n",
        "    label = group_id\n",
        "\n",
        "    #lay duong dan toi cac file (ten_file, duong_dan)\n",
        "    path_to_group = parent_dir + group_name\n",
        "    files = [(file_name, path_to_group + '/'  + file_name ) for file_name in listdir(path_to_group)]\n",
        "    files.sort() #sap xep cac tuple teo file_name\n",
        "\n",
        "    #Doc DL tu cac file trong group va xu li:\n",
        "    for file_name, file_path in files:\n",
        "      with open(file_path, encoding='utf8', errors='ignore') as f:\n",
        "        text = f.read()#chuyen het ve chu thuong\n",
        "        \n",
        "        #Loai bo cac ki tu khong co trong [a-zA-Z0-9] va khong phai stop word\n",
        "        words_in_file = [stemmer.stem(word) for word in re.split('\\W+', text) if word not in stop_words]\n",
        "\n",
        "        #ket hop cac tu lai thanh 1 xau (cac tu cach nhau boi 1 khoang trang cach)\n",
        "        content = ' '.join(words_in_file)\n",
        "\n",
        "        #day du lieu vao data:\n",
        "        data.append(str(label) + '<fff>' + file_name + '<fff>' + content)\n",
        "      f.close()\n",
        "\n",
        "  return data\n",
        "\n",
        "#Ghi ket qua da thu thap duoc vao file:\n",
        "\n",
        "def write_processed_data_to_file():\n",
        "  with open('20news-train-processed.txt', 'w') as f:\n",
        "    f.write('\\n'.join(train_data)) #chuyen mang cac van ban thanh file gom cac van ban, moi ban ban tren 1 dong\n",
        "  f.close()\n",
        "\n",
        "  with open('20news-test-processed.txt', 'w') as f:\n",
        "    f.write('\\n'.join(test_data)) \n",
        "  f.close()\n",
        "\n",
        "  with open('20news-full-processed.txt', 'w') as f:\n",
        "    f.write('\\n'.join(full_data))\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21XDcRYxRVsX",
        "colab_type": "text"
      },
      "source": [
        "#Tao tu dien - Tinh toan if-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP8i9Yf1RbhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tao tu dien - Su dung file full-precessed vua xu li duoc:\n",
        "def create_dictionary():\n",
        "  data_path = '20news-full-processed.txt'\n",
        "\n",
        "  with open(data_path) as file:\n",
        "    lines = file.read().split('\\n') #moi van ban la mot phan tu cua mang lines\n",
        "  file.close()\n",
        "\n",
        "  corpus_size = len(lines) #kich thuoc tap D - tong so van ban\n",
        "\n",
        "  #dem so van ban ma moi tu trong tu dien xuat hien\n",
        "  from collections import defaultdict\n",
        "  doc_count = defaultdict(int) #{tu: xuat hien trong bao nhieu van ban}\n",
        "\n",
        "  for line in lines:\n",
        "    features = line.split('<fff>') #tach lay phan van ban theo dinh dang truoc do da luu\n",
        "    text = features[-1]\n",
        "    words = list(set(text.split())) #thu duoc danh sach cac tu co trong mot van ban nao do\n",
        "    for word in words:\n",
        "      doc_count[word] +=1\n",
        "\n",
        "  return doc_count #tu - so van ban ma no xuat hien\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6zz0sv1uESw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tinh idf cua tu:\n",
        "import numpy as np\n",
        "def compute_idf(freq, corpus_size):\n",
        "  return np.log10(corpus_size*1./freq)\n",
        "\n",
        "def compute_word_idfs(doc_count):\n",
        "  words_idfs = [(word, compute_idf(doc_freq, corpus_size))\\\n",
        "                for word, doc_freq in zip(doc_count.keys(), doc_count.values())\\\n",
        "                if doc_freq > 10 and not word.isdigit()]\n",
        "  words_idfs.sort(key = lambda w:w[0])\n",
        "\n",
        "  vocab_size = len(words_idfs)\n",
        "  print(\"So tu trong tu dien: \", vocab_size)\n",
        "\n",
        "  #Ghi ket qua tinh toan idfs ra file:\n",
        "  with open('words_idfs.txt', 'w') as f:\n",
        "    f.write('\\n'.join([word + '<fff>' + str(idf) for word, idf in words_idfs]))\n",
        "\n",
        "  return words_idfs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gbkUFtjKY4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Tinh tf-idf:\n",
        "def get_tf_idf(data_path):\n",
        "\n",
        "  #doc tu file words_idfs da luu truoc do de lay thong tin ve cac tu - tan suat xuat hien trong cac van ban\n",
        "  with open('words_idfs.txt') as f:\n",
        "    words_idfs = [(line.split('<fff>')[0], float(line.split('<fff>')[1]))\\\n",
        "                 for line in f.read().split('\\n')]\n",
        "    word_IDs  = dict([(word, index) for index, (word, idfs) in enumerate(words_idfs)])\n",
        "    idfs = dict(words_idfs) #tu dien chua cac tu va tan suat xuat hien cua chung\n",
        "\n",
        "  #doc tu file full-processed de lay toan bo van ban ra xem xet:\n",
        "  with open(data_path) as f:\n",
        "    documents =[(line.split('<fff>')[0],\\\n",
        "                 line.split('<fff>')[1], \\\n",
        "                 line.split('<fff>')[2])\\\n",
        "                for line in  f.read().split('\\n')]\n",
        "              \n",
        "  data_tf_idf = []\n",
        "\n",
        "  #xu li tung van ban\n",
        "  for document in documents:\n",
        "    label, doc_id, text = document\n",
        "    words = [word for word in text.split() if word in idfs] #chi xet cac tu co tan suat > 10, khong phai so\n",
        "    word_set = list(set(words))\n",
        "    max_term_freq = max([words.count(word) for word in word_set])\n",
        "    words_tfidfs = [] #mang chua tf-idf cua cac tu \n",
        "    sum_squares = 0.0\n",
        "\n",
        "    #xet tf_idf cua tung tu trong van ban\n",
        "    for word in word_set:\n",
        "      term_freq = words.count(word)\n",
        "      tf_idf_value = term_freq*1./max_term_freq*idfs[word]\n",
        "      words_tfidfs.append((word_IDs[word], tf_idf_value))\n",
        "      sum_squares +=tf_idf_value**2 #de chuan hoa \n",
        "\n",
        "    words_tfidfs_normalized = [str(index) + ':'\\\n",
        "                               + str(tf_idf_value/np.sqrt(sum_squares))\\\n",
        "                               for index, tf_idf_value in words_tfidfs]\n",
        "\n",
        "    sparse_rep = ' '.join(words_tfidfs_normalized)\n",
        "    data_tf_idf.append((label, doc_id, sparse_rep))\n",
        "\n",
        "  return data_tf_idf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thKXNeKoLsWq",
        "colab_type": "text"
      },
      "source": [
        "#Ham main:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygfBYT0OLrgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  #goi thuc hien ham, tai day train_data, test_data tuong ung la 2 mang chua cac phan tu la cac van ban cua tap train, test tuong ung - cung voi nhan\n",
        "  train_dir, test_dir, parent_dirs, list_group_name = get_list_dirs_and_newsgroup()\n",
        "  train_data = collect_data(train_dir, list_group_name)\n",
        "  test_data = collect_data(test_dir, list_group_name)\n",
        "  full_data = train_data + test_data #noi hai list tren lai thanh list full - gom toan bo cac van ban\n",
        "\n",
        "  write_processed_data_to_file() #Ghi kq ra file\n",
        "\n",
        "  #Tao tu dien va tinh toan TF-IDF\n",
        "  dictionary = create_dictionary()\n",
        "  compute_word_idfs() #tinh idfs cua cac tu\n",
        "  data_tf_idf = get_tf_idf(data_path = '20news-full-processed.txt') #tinh tf_idf tren toan bo du lieu:\n",
        "\n",
        "\n",
        "  #Ghi ket qua cuoi cung ra file:\n",
        "  with open('data_tf_idf.txt', 'w') as f:\n",
        "    f.write('\\n'.join([d_tf_idf[0] + '<fff>' + d_tf_idf[1] + '<fff>' + d_tf_idf[2] for d_tf_idf in data_tf_idf]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}